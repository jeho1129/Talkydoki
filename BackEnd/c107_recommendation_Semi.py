# -*- coding: utf-8 -*-
"""C107_Recommendation_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcrbwnuXvv6ZpuU4QDe2v3JBqdIeFw_N
"""

from sklearn.metrics.pairwise import cosine_similarity

import math
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader

# 사용자 / 기사 / 단어에 대한 더미 데이터 생성
users = ['user' + str(i) for i in range(1, 1001)]
categories = ['사회', '경제', '정치', '과학', '문화']
articles = [f"{category}기사{i}" for category in categories for i in range(1, 201)]
words = [f"{category}_{i}" for category in categories for i in range(1, 201)]

article_word_df = pd.DataFrame(data = np.zeros((1000, 1000), dtype = int), index = articles, columns = words)
user_word_df = pd.DataFrame(data = np.zeros((1000, 1000), dtype = int), index = users, columns = words)

# 사용자 별로 선호하는 카테고리 설정
user_preferences = [category for category in categories for _ in range(200)]

# 사용자 별 경향성을 반영하여 선호하는 카테고리의 단어를 더 많이 학습
for i, user in enumerate(users):
    preferred_category = user_preferences[i]
    preferred_words = [word for word in words if preferred_category in word]
    non_preferred_words = [word for word in words if preferred_category not in word]

    # 선호하는 카테고리의 단어 중 10~15%를 무작위로 선택하여 학습 횟수를 0으로 설정
    num_to_exclude = int(len(preferred_words) * np.random.uniform(0.1, 0.15))
    excluded_words = np.random.choice(preferred_words, size = num_to_exclude, replace = False)

    for word in preferred_words:
        if word not in excluded_words:
            user_word_df.at[user, word] = np.random.randint(5, 11)
        else:
            user_word_df.at[user, word] = 0

    for word in non_preferred_words:
        user_word_df.at[user, word] = np.random.randint(0, 5)

article_categories = [category for category in categories for _ in range(200)]

# 기사별 카테고리에 따라 TF-IDF 값 조정
for i, article in enumerate(article_word_df.index):
    category = article_categories[i]
    category_words = [word for word in article_word_df.columns if category in word]
    non_category_words = [word for word in article_word_df.columns if category not in word]

    # 해당 카테고리의 단어 중 10~15%를 무작위로 선택하여 TF-IDF 값을 0으로 설정
    num_to_exclude = int(len(category_words) * np.random.uniform(0.1, 0.15))
    excluded_words = np.random.choice(category_words, size = num_to_exclude, replace = False)

    for word in category_words:
        if word not in excluded_words:
            article_word_df.at[article, word] = np.random.uniform(0.5, 1)
        else:
            article_word_df.at[article, word] = 0  # 명시적으로 0으로 설정

    for word in non_category_words:
        article_word_df.at[article, word] = np.random.uniform(0, 0.5)

user_word_df

article_word_df

plt.rcParams['figure.figsize'] = (10, 8)  # 시각화 크기 설정
plt.rcParams['figure.dpi'] = 100  # 해상도 설정

# user_word_df 데이터프레임에 대한 Clustermap 생성
sns.clustermap(user_word_df, method = 'average', metric = 'euclidean', figsize = (10, 10), cmap = 'viridis')
plt.title("User-Word Preferences Clustermap")
plt.show()

# article_word_df 데이터프레임에 대한 Clustermap 생성
sns.clustermap(article_word_df, method = 'average', metric = 'euclidean', figsize = (10, 10), cmap = 'viridis')
plt.title("Article-Word Categories Clustermap")
plt.show()

# 데이터 간 Scale을 맞추기 위하여 Min-Max 정규화
user_word_df_norm = (user_word_df - user_word_df.min()) / (user_word_df.max() - user_word_df.min())
user_word_df_norm

# 뉴스 기사 추천 : Cosine 유사도 기반 Content-Based Filtering Algorithm
# 코사인 유사도 기반으로 사용자와 관련성이 높은 기사를 추천하는 방법

# 사용자 - 기사 간 코사인 유사도 계산
cosine_sim = cosine_similarity(user_word_df_norm, article_word_df)

# 코사인 유사도를 DataFrame으로 변환
cosine_sim_df = pd.DataFrame(cosine_sim, columns = articles, index = users)
cosine_sim_df

# 각 사용자에 대한 추천 리스트 생성하여 추천 점수가 높은 기사 추천

# 실제로 구현할 때는 해당 기사가 이미 읽었던 기사인지, 아닌지 검증하는 과정 추가 필요

recommendations = {}
for user in users:
    user_data = cosine_sim_df.loc[user].sort_values(ascending = False)
    recommendations[user] = user_data.index.values.tolist()[:3]

print(recommendations)

# 단어 추천 방법 1 : 해당 사용자가 아직 학습하지 않은 단어들 중 해당 사용자와 유사한 사용자들이 많이 학습한 단어를 추천하는 알고리즘

# 사용자 간 유사도 계산
user_similarity = cosine_similarity(user_word_df_norm)

# 유사도를 DataFrame으로 변환
user_similarity_df = pd.DataFrame(user_similarity, index = users, columns = users)
user_similarity_df

# 사용자가 아직 학습하지 않은 단어 찾기
unlearned_words = user_word_df_norm.apply(lambda x: x == 0)

# 추천 단어 리스트 생성
word_recommendations = {}
for user in users:
    # 사용자와 유사한 사용자들 찾기
    similar_users = user_similarity_df[user].sort_values(ascending=False).index[1:]

    # 유사한 사용자들이 많이 학습한 단어 찾기
    similar_users_words = user_word_df_norm.loc[similar_users].mean().sort_values(ascending=False)

    # 사용자가 아직 학습하지 않은 단어 중에서 유사한 사용자들이 많이 학습한 단어 선택
    user_unlearned_words = unlearned_words.loc[user]
    recommended_words = similar_users_words[user_unlearned_words].sort_values(ascending=False)

    # 상위 3개 단어 추천
    word_recommendations[user] = recommended_words.index[:3].tolist()

print(word_recommendations)

# 단어 추천 방법 2 : Matrix Factorization
# 행렬을 두 개의 저차원 행렬로 분해하여 각각 사용자와 단어의 잠재적 특성을 나타냄
# 잠재적 특성을 기반으로 사용자와 단어 사이의 관계를 예측

# 행렬 분해 모델 클래스 정의
class MatrixFactorization(nn.Module):
    def __init__(self, num_users, num_items, latent_dim, dropout_rate=0.8, l2=0.01):
        super(MatrixFactorization, self).__init__()
        self.user_embedding = nn.Embedding(num_users, latent_dim)
        self.item_embedding = nn.Embedding(num_items, latent_dim)
        self.user_bias = nn.Embedding(num_users, 1)
        self.item_bias = nn.Embedding(num_items, 1)
        self.dropout = nn.Dropout(dropout_rate)
        self.l2 = l2

        nn.init.normal_(self.user_embedding.weight, mean=0.0, std=0.01)
        nn.init.normal_(self.item_embedding.weight, mean=0.0, std=0.01)
        nn.init.zeros_(self.user_bias.weight)
        nn.init.zeros_(self.item_bias.weight)

    def forward(self, user_indices, item_indices):
        user_latent = self.dropout(self.user_embedding(user_indices))
        item_latent = self.dropout(self.item_embedding(item_indices))
        user_bias = self.user_bias(user_indices).squeeze()
        item_bias = self.item_bias(item_indices).squeeze()

        prediction = torch.sum(user_latent * item_latent, dim=1) + user_bias + item_bias
        return prediction

    def loss(self, prediction, target):
        return F.mse_loss(prediction, target)

# 데이터셋 클래스 정의
class UserWordDataset(Dataset):
    def __init__(self, data):
        self.users = {user: idx for idx, user in enumerate(data.index)}
        self.items = {item: idx for idx, item in enumerate(data.columns)}
        self.ratings = data.values.flatten().astype(np.float32)

        user_indices = [self.users[user] for user in data.index.repeat(len(data.columns))]
        item_indices = [self.items[item] for item in np.tile(data.columns, len(data.index))]

        self.user_indices = np.array(user_indices)
        self.item_indices = np.array(item_indices)

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        user = torch.tensor(self.user_indices[idx], dtype=torch.long)
        item = torch.tensor(self.item_indices[idx], dtype=torch.long)
        rating = torch.tensor(self.ratings[idx], dtype=torch.float)
        return user, item, rating

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# RMSE 계산 함수
def compute_rmse(predictions, ratings):
    return torch.sqrt(torch.mean((predictions - ratings) ** 2))

# 데이터 로딩
dataset = UserWordDataset(user_word_df)  # 데이터셋을 디바이스로 옮김
train_size = int(0.8 * len(dataset))
test_valid_size = len(dataset) - train_size
valid_size = test_valid_size // 2
test_size = valid_size

# 데이터셋 분할
train_dataset, test_valid_dataset = torch.utils.data.random_split(dataset, [train_size, test_valid_size])
test_dataset, valid_dataset = torch.utils.data.random_split(test_valid_dataset, [test_size, valid_size])

# 데이터 로더 설정
train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)
valid_loader = DataLoader(valid_dataset, batch_size = 64, shuffle = False)
test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)

# 모델 초기화
num_users, num_items = len(user_word_df.index), len(user_word_df.columns)
model = MatrixFactorization(num_users, num_items, latent_dim = 270, dropout_rate = 0.8).to(device)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters(), lr =  0.0008)

train_losses_epochs = []
val_losses_epochs = []
val_rmses_epochs = []

# 학습 진행
best_val_loss = float('inf')
for epoch in range(20):
    model.train()
    train_losses = []
    for user_indices, item_indices, ratings in train_loader:
        user_indices, item_indices, ratings = user_indices.to(device), item_indices.to(device), ratings.to(device)
        prediction = model(user_indices, item_indices)
        loss = model.loss(prediction, ratings)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    avg_train_loss = np.mean(train_losses)
    print(f"Epoch {epoch+1} Training Loss: {avg_train_loss}")

    # 검증 손실 계산
    model.eval()
    total_loss = 0
    total_rmse = 0
    with torch.no_grad():
        for user_indices, item_indices, ratings in valid_loader:
            user_indices, item_indices, ratings = user_indices.to(device), item_indices.to(device), ratings.to(device)
            prediction = model(user_indices, item_indices)
            loss = model.loss(prediction, ratings)
            rmse = compute_rmse(prediction, ratings)

            total_loss += loss.item()
            total_rmse += rmse.item()

    avg_val_loss = total_loss / len(valid_loader)
    avg_val_rmse = total_rmse / len(valid_loader)
    train_losses_epochs.append(avg_train_loss)
    val_losses_epochs.append(avg_val_loss)
    val_rmses_epochs.append(avg_val_rmse)
    print(f"Epoch {epoch+1} Validation Loss: {avg_val_loss}, RMSE: {avg_val_rmse}")

    # 모델 저장
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        save_content = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': best_val_loss,
        }
        torch.save(save_content, 'best_model.pth')

# 테스트 데이터셋을 사용한 모델 평가
model.eval()
total_test_loss = 0
total_test_rmse = 0
with torch.no_grad():
    for user_indices, item_indices, ratings in test_loader:
        user_indices, item_indices, ratings = user_indices.to(device), item_indices.to(device), ratings.to(device)
        prediction = model(user_indices, item_indices)
        loss = model.loss(prediction, ratings)
        rmse = compute_rmse(prediction, ratings)

        total_test_loss += loss.item()
        total_test_rmse += rmse.item()
avg_test_loss = total_test_loss / len(test_loader)
avg_test_rmse = total_test_rmse / len(test_loader)
print(f"Final Test Loss: {avg_test_loss}, RMSE: {avg_test_rmse}")

for user_index in range(num_users):
    untrained_item_indices = []
    untrained_item_names = user_word_df.iloc[user_index][user_word_df.iloc[user_index] == 0].index.tolist()
    for item_name in untrained_item_names:
        item_index = dataset.items[item_name]
        untrained_item_indices.append(item_index)

    untrained_item_indices_tensor = torch.tensor(untrained_item_indices, dtype=torch.long, device=device)
    # 예측을 수행
    predictions = model(torch.tensor([user_index] * len(untrained_item_indices), device=device), untrained_item_indices_tensor)
    # 가장 높은 예측 점수를 가진 아이템 인덱스를 추천
    recommended_item_index = untrained_item_indices[torch.argmax(predictions).item()]
    # 추천된 아이템 인덱스를 다시 아이템 이름으로 변환하여 출력
    recommended_item_name = list(dataset.items.keys())[list(dataset.items.values()).index(recommended_item_index)]
    print(f"User {user_index} recommended item: {recommended_item_name}")

plt.figure(figsize=(10, 7))
plt.subplot(2, 1, 1)
plt.plot(train_losses_epochs, label='Train Loss')
plt.plot(val_losses_epochs, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(val_rmses_epochs, label='Validation RMSE', color='orange')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.title('Validation RMSE')
plt.legend()

plt.tight_layout()
plt.show()