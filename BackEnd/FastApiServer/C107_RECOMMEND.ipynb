{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Za-iEoSwDdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5040283b-b48e-4b92-e4c1-c5f62f8fc9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9IR3pa-wca-",
        "outputId": "875fba6d-29bf-45ac-e80d-5234488f0e29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.3.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-UQX-OYpwSHT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 경로\n",
        "user_word_df = '/content/drive/MyDrive/user_word_df.csv'\n",
        "\n",
        "# CSV 파일을 DataFrame으로 읽기\n",
        "user_word_df = pd.read_csv(user_word_df)\n",
        "user_word_df.drop(columns=user_word_df.columns[0], axis=1, inplace=True)\n",
        "\n",
        "# DataFrame 확인\n",
        "print(user_word_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmH-swTHwTCx",
        "outputId": "fbc380c5-d0ee-4f57-bb2d-99be3c61ee2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Unnamed: 0  1  2    3  4  5  6  7  8  9  ...  4870  4871  4872  4873  \\\n",
            "0        사회기사1  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "1        사회기사2  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "2        사회기사3  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "3        사회기사4  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "4        사회기사5  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "..         ... .. ..  ... .. .. .. .. .. ..  ...   ...   ...   ...   ...   \n",
            "335    국제기사336  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "336   스포츠기사337  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "337   스포츠기사338  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "338    사회기사339  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "339   스포츠기사340  0  0  0.0  0  0  0  0  0  0  ...     0     0     0     0   \n",
            "\n",
            "     4874  4875  4876  4877  4878  4879  \n",
            "0       0     0     0     0     0     0  \n",
            "1       0     0     0     0     0     0  \n",
            "2       0     0     0     0     0     0  \n",
            "3       0     0     0     0     0     0  \n",
            "4       0     0     0     0     0     0  \n",
            "..    ...   ...   ...   ...   ...   ...  \n",
            "335     0     0     0     0     0     0  \n",
            "336     0     0     0     0     0     0  \n",
            "337     0     0     0     0     0     0  \n",
            "338     0     0     0     0     0     0  \n",
            "339     0     0     0     0     0     0  \n",
            "\n",
            "[340 rows x 4880 columns]\n",
            "      1  2  3  4  5  6  7  8  9  10  ...  4870  4871  4872  4873  4874  4875  \\\n",
            "0     1  2  4  0  2  0  4  0  1   3  ...     2     4     1     2     3     0   \n",
            "1     1  4  2  3  3  0  3  1  4   0  ...     3     2     3     4     0     2   \n",
            "2     1  0  1  3  2  0  3  0  1   4  ...     4     2     4     4     4     4   \n",
            "3     4  0  0  3  3  1  4  0  2   4  ...     4     3     1     3     3     0   \n",
            "4     1  1  0  3  1  2  4  3  1   1  ...     2     2     0     3     3     2   \n",
            "...  .. .. .. .. .. .. .. .. ..  ..  ...   ...   ...   ...   ...   ...   ...   \n",
            "1001  1  3  0  4  1  1  3  0  3   3  ...     2     2     3     0     4     4   \n",
            "1002  1  1  0  1  3  4  2  3  2   0  ...     3     2     3     0     1     1   \n",
            "1003  0  3  3  4  3  4  0  4  2   3  ...     1     3     1     0     1     3   \n",
            "1004  0  3  2  0  0  4  1  2  2   0  ...     0     2     4     4     3     2   \n",
            "1005  0  3  3  4  3  0  2  2  4   4  ...     0     1     3     1     1     4   \n",
            "\n",
            "      4876  4877  4878  4879  \n",
            "0        1     2     2     4  \n",
            "1        3     1     4     0  \n",
            "2        3     3     2     1  \n",
            "3        1     2     1     0  \n",
            "4        1     1     3     4  \n",
            "...    ...   ...   ...   ...  \n",
            "1001     0     4     4     0  \n",
            "1002     1     2     4     0  \n",
            "1003     2     2     0     0  \n",
            "1004     4     4     0     1  \n",
            "1005     2     3     0     4  \n",
            "\n",
            "[1006 rows x 4879 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 추천 방법 2 : Matrix Factorization\n",
        "# 행렬을 두 개의 저차원 행렬로 분해하여 각각 사용자와 단어의 잠재적 특성을 나타냄\n",
        "# 잠재적 특성을 기반으로 사용자와 단어 사이의 관계를 예측\n",
        "\n",
        "# 행렬 분해 모델 클래스 정의\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, num_users, num_items, latent_dim, dropout_rate=0.8, l2=0.01):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, latent_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, latent_dim)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.l2 = l2\n",
        "\n",
        "        nn.init.normal_(self.user_embedding.weight, mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.item_embedding.weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.user_bias.weight)\n",
        "        nn.init.zeros_(self.item_bias.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_latent = self.dropout(self.user_embedding(user_indices))\n",
        "        item_latent = self.dropout(self.item_embedding(item_indices))\n",
        "        user_bias = self.user_bias(user_indices).squeeze()\n",
        "        item_bias = self.item_bias(item_indices).squeeze()\n",
        "\n",
        "        prediction = torch.sum(user_latent * item_latent, dim=1) + user_bias + item_bias\n",
        "        return prediction\n",
        "\n",
        "    def loss(self, prediction, target):\n",
        "        mse_loss = F.mse_loss(prediction, target.float())\n",
        "        l2_loss = sum(torch.norm(param) for param in self.parameters())\n",
        "        total_loss = mse_loss + self.l2 * l2_loss\n",
        "        return total_loss\n",
        "\n",
        "class UserWordDataset(Dataset):\n",
        "    def __init__(self, user_word_matrix):\n",
        "        self.user_word_matrix = user_word_matrix.values\n",
        "        self.num_users, self.num_items = user_word_matrix.shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_users * self.num_items\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_id = idx // self.num_items\n",
        "        item_id = idx % self.num_items\n",
        "        rating = self.user_word_matrix[user_id, item_id]\n",
        "        return user_id, item_id, torch.tensor(rating, dtype=torch.float)"
      ],
      "metadata": {
        "id": "p2zlKb1lwUQI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = UserWordDataset(user_word_df)\n",
        "\n",
        "df = user_word_df\n",
        "\n",
        "def split_data(dataset, batch_size=64):\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_dataset = UserWordDataset(train_df)\n",
        "    test_dataset = UserWordDataset(test_df)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = split_data(dataset)\n",
        "\n",
        "# 베이지안 최적화를 위한 목적 함수 정의\n",
        "def objective(latent_dim, lr, dropout_rate, l2):\n",
        "    print(f\"하이퍼파라미터: latent_dim={latent_dim}, lr={lr}, dropout_rate={dropout_rate}, l2={l2}\")\n",
        "    num_users, num_items = df.shape\n",
        "    model = MatrixFactorization(num_users, num_items, int(latent_dim), dropout_rate, l2).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # 학습 과정\n",
        "    for epoch in range(5):  # 에포크 수 조정 가능\n",
        "        epoch_losses = []  # 에포크별 평균 loss를 계산하기 위해 사용\n",
        "        for user_ids, item_ids, ratings in train_loader:\n",
        "            user_ids, item_ids, ratings = user_ids.to(device), item_ids.to(device), ratings.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(user_ids, item_ids)\n",
        "            loss = model.loss(outputs, ratings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "        epoch_avg_loss = np.mean(epoch_losses)\n",
        "        print(f\"에포크 {epoch+1}, 평균 Loss: {epoch_avg_loss}\")\n",
        "\n",
        "    # 검증 과정 - 여기서는 간단하게 훈련 데이터로 대체\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for user_ids, item_ids, ratings in test_loader:\n",
        "            user_ids, item_ids, ratings = user_ids.to(device), item_ids.to(device), ratings.to(device)\n",
        "            outputs = model(user_ids, item_ids)\n",
        "            loss = model.loss(outputs, ratings)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    avg_loss = np.mean(losses)\n",
        "    print(f\"검증 Loss: {avg_loss}\")\n",
        "    return -avg_loss  # 최소화 대신 최대화를 위해 음수 반환\n",
        "\n",
        "# 베이지안 최적화 실행\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds={\n",
        "        \"latent_dim\": (50, 200),\n",
        "        \"lr\": (1e-4, 1e-2),\n",
        "        \"dropout_rate\": (0.1, 0.5),\n",
        "        \"l2\": (1e-5, 1e-3),\n",
        "    },\n",
        "    random_state=1,\n",
        ")\n",
        "optimizer.maximize(init_points=2, n_iter=5)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(optimizer.max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI4X6qcTwVyj",
        "outputId": "ab1ad56a-64cd-4d07-87a8-862c8a1a25e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | dropou... |    l2     | latent... |    lr     |\n",
            "-------------------------------------------------------------------------\n",
            "하이퍼파라미터: latent_dim=50.017156222601734, lr=0.003093092469055214, dropout_rate=0.2668088018810296, l2=0.0007231212485077365\n",
            "에포크 1, 평균 Loss: 2.3692015162096665\n",
            "에포크 2, 평균 Loss: 2.3253165266306226\n",
            "에포크 3, 평균 Loss: 2.3213448965502597\n",
            "에포크 4, 평균 Loss: 2.319120397033755\n",
            "에포크 5, 평균 Loss: 2.318329994701408\n",
            "검증 Loss: 2.25722716134864\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-2.257   \u001b[0m | \u001b[0m0.2668   \u001b[0m | \u001b[0m0.0007231\u001b[0m | \u001b[0m50.02    \u001b[0m | \u001b[0m0.003093 \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=77.93903170665064, lr=0.003521051197726173, dropout_rate=0.15870235632684523, l2=0.00010141520882110982\n",
            "에포크 1, 평균 Loss: 2.3865921436333184\n",
            "에포크 2, 평균 Loss: 2.3736514520429646\n",
            "에포크 3, 평균 Loss: 2.375516441701849\n",
            "에포크 4, 평균 Loss: 2.374292550673093\n",
            "에포크 5, 평균 Loss: 2.3748604029656746\n",
            "검증 Loss: 2.2864408362066593\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-2.286   \u001b[0m | \u001b[0m0.1587   \u001b[0m | \u001b[0m0.0001014\u001b[0m | \u001b[0m77.94    \u001b[0m | \u001b[0m0.003521 \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=51.329069011626025, lr=0.004385583279584944, dropout_rate=0.15322433537213645, l2=0.0004117445196891262\n",
            "에포크 1, 평균 Loss: 2.4177073304578833\n",
            "에포크 2, 평균 Loss: 2.3992767341900154\n",
            "에포크 3, 평균 Loss: 2.397075906787568\n",
            "에포크 4, 평균 Loss: 2.3956025376022003\n",
            "에포크 5, 평균 Loss: 2.3944463115909067\n",
            "검증 Loss: 2.327308597642106\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-2.327   \u001b[0m | \u001b[0m0.1532   \u001b[0m | \u001b[0m0.0004117\u001b[0m | \u001b[0m51.33    \u001b[0m | \u001b[0m0.004386 \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=132.1707418678506, lr=0.007467595657594839, dropout_rate=0.27702292698218495, l2=0.0004193360008547263\n",
            "에포크 1, 평균 Loss: 4.455689219414547\n",
            "에포크 2, 평균 Loss: 4.571448339048373\n",
            "에포크 3, 평균 Loss: 4.5689103075748205\n",
            "에포크 4, 평균 Loss: 4.568021695966628\n",
            "에포크 5, 평균 Loss: 4.56854579334845\n",
            "검증 Loss: 3.504717199856585\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-3.505   \u001b[0m | \u001b[0m0.277    \u001b[0m | \u001b[0m0.0004193\u001b[0m | \u001b[0m132.2    \u001b[0m | \u001b[0m0.007468 \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=95.31385921785993, lr=0.01, dropout_rate=0.5, l2=0.001\n",
            "에포크 1, 평균 Loss: 5.832946695124679\n",
            "에포크 2, 평균 Loss: 5.923911607880258\n",
            "에포크 3, 평균 Loss: 5.9222356042791455\n",
            "에포크 4, 평균 Loss: 5.92755991049614\n",
            "에포크 5, 평균 Loss: 5.918328738734988\n",
            "검증 Loss: 3.4142723682097027\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-3.414   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.001    \u001b[0m | \u001b[0m95.31    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=67.67865540758136, lr=0.0001, dropout_rate=0.5, l2=0.001\n",
            "에포크 1, 평균 Loss: 3.6528010806593447\n",
            "에포크 2, 평균 Loss: 2.1765864699052537\n",
            "에포크 3, 평균 Loss: 2.0845876129344196\n",
            "에포크 4, 평균 Loss: 2.080455615077699\n",
            "에포크 5, 평균 Loss: 2.079889875407195\n",
            "검증 Loss: 2.0794349673506503\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m-2.079   \u001b[0m | \u001b[95m0.5      \u001b[0m | \u001b[95m0.001    \u001b[0m | \u001b[95m67.68    \u001b[0m | \u001b[95m0.0001   \u001b[0m |\n",
            "하이퍼파라미터: latent_dim=200.0, lr=0.01, dropout_rate=0.5, l2=1e-05\n",
            "에포크 1, 평균 Loss: 18.162942570526592\n",
            "에포크 2, 평균 Loss: 19.451625955115915\n",
            "에포크 3, 평균 Loss: 19.43911326673532\n",
            "에포크 4, 평균 Loss: 19.457411100940146\n",
            "에포크 5, 평균 Loss: 19.460133608891386\n",
            "검증 Loss: 6.566021953372212\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-6.566   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m200.0    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
            "=========================================================================\n",
            "{'target': -2.0794349673506503, 'params': {'dropout_rate': 0.5, 'l2': 0.001, 'latent_dim': 67.67865540758136, 'lr': 0.0001}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적 하이퍼파라미터 사용\n",
        "best_params = optimizer.max['params']\n",
        "print(\"최적의 하이퍼파라미터:\", best_params)\n",
        "\n",
        "latent_dim = int(best_params['latent_dim'])\n",
        "lr = best_params['lr']\n",
        "dropout_rate = best_params['dropout_rate']\n",
        "l2 = best_params['l2']\n",
        "\n",
        "# 최적 하이퍼파라미터로 모델 초기화\n",
        "num_users, num_items = df.shape\n",
        "model = MatrixFactorization(num_users, num_items, latent_dim, dropout_rate, l2).to(device)\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 학습 과정\n",
        "num_epochs = 10  # 최종 모델 학습을 위해 에포크 수 조정\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    for user_ids, item_ids, ratings in train_loader:\n",
        "        user_ids, item_ids, ratings = user_ids.to(device), item_ids.to(device), ratings.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(user_ids, item_ids)\n",
        "        loss = model.loss(outputs, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "    epoch_avg_loss = np.mean(epoch_losses)\n",
        "    print(f\"에포크 {epoch+1}, 평균 Loss: {epoch_avg_loss}\")\n",
        "\n",
        "model_path = 'matrix_factorization_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(\"모델이 '{}'에 저장되었습니다.\".format(model_path))\n",
        "\n",
        "# 테스트 과정\n",
        "model.eval()\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "    for user_ids, item_ids, ratings in test_loader:\n",
        "        user_ids, item_ids, ratings = user_ids.to(device), item_ids.to(device), ratings.to(device)\n",
        "        outputs = model(user_ids, item_ids)\n",
        "        loss = model.loss(outputs, ratings)\n",
        "        test_losses.append(loss.item())\n",
        "\n",
        "avg_test_loss = np.mean(test_losses)\n",
        "print(f\"테스트 데이터에 대한 평균 Loss: {avg_test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOn-cfRFAbKJ",
        "outputId": "b87602d5-cd80-4600-caff-25f95e1b98f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 하이퍼파라미터: {'dropout_rate': 0.5, 'l2': 0.001, 'latent_dim': 67.67865540758136, 'lr': 0.0001}\n",
            "에포크 1, 평균 Loss: 3.6533404152728606\n",
            "에포크 2, 평균 Loss: 2.1768567823615697\n",
            "에포크 3, 평균 Loss: 2.084598619954662\n",
            "에포크 4, 평균 Loss: 2.0804560221354227\n",
            "에포크 5, 평균 Loss: 2.079886250848095\n",
            "에포크 6, 평균 Loss: 2.079520409291547\n",
            "에포크 7, 평균 Loss: 2.079174592707621\n",
            "에포크 8, 평균 Loss: 2.0788387768948486\n",
            "에포크 9, 평균 Loss: 2.0785204534451442\n",
            "에포크 10, 평균 Loss: 2.0781882823426714\n",
            "모델이 'matrix_factorization_model.pth'에 저장되었습니다.\n",
            "테스트 데이터에 대한 평균 Loss: 2.078276413276598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_info = {\n",
        "    'state_dict': model.state_dict(),\n",
        "    'hyperparams': {\n",
        "        'latent_dim': latent_dim,\n",
        "        'lr': lr,\n",
        "        'dropout_rate': dropout_rate,\n",
        "        'l2': l2\n",
        "    },\n",
        "    'num_users': num_users,\n",
        "    'num_items': num_items\n",
        "}\n",
        "\n",
        "# 정보를 파일로 저장\n",
        "model_path = 'matrix_factorization_model_with_hyperparams.pth'\n",
        "torch.save(model_info, model_path)\n",
        "print(\"모델과 하이퍼파라미터가 '{}'에 저장되었습니다.\".format(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPt0CwrKf4se",
        "outputId": "db3fba3c-de3e-4d62-85c5-d7afee75dff9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델과 하이퍼파라미터가 'matrix_factorization_model_with_hyperparams.pth'에 저장되었습니다.\n"
          ]
        }
      ]
    }
  ]
}